{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS7r4B7yuOey"
   },
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# Object Detection, Tracking, and Counting with YOLOv8 and SORT Using OOP Approach\n",
    "\n",
    "</div>\n",
    "\n",
    "Welcome to this Google Colab notebook, which demonstrates how to perform object detection, tracking, and counting on both video files and static images using the YOLOv8 object detection algorithm and the SORT (Simple Online and Realtime Tracking) algorithm for object tracking, all implemented using an Object-Oriented Programming (OOP) approach.\n",
    "\n",
    "In this notebook, we will be using the YOLOv8 algorithm to predict the class probabilities and bounding boxes of multiple objects in an image or video frame. We will also be using the SORT algorithm to track the objects detected by YOLOv8 across multiple frames of a video or a sequence of images. SORT is a simple but effective algorithm that is designed to work in real-time and to handle noisy detections and occlusions.\n",
    "\n",
    "By using an OOP approach, we can organize our code into classes and methods that encapsulate the functionality of different parts of the object detection, tracking, and counting pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoZrnFsqLDR6"
   },
   "source": [
    "## Importing libraries, modules and files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwFSZL50YWFh"
   },
   "source": [
    "### importing files from my github repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_FEoVtf-igA",
    "outputId": "75af878e-c798-46d5-976e-d41eb73a4bb7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Object-tracking-and-counting-using-YOLOV8' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/mohamedamine99/Object-tracking-and-counting-using-YOLOV8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ywW_Y4qYmI2"
   },
   "source": [
    "### importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "hrodd0lKPiDz",
    "outputId": "165180b6-8172-4b7d-cb09-50692a21136d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\vinil\\\\Downloads\\\\Object-tracking-and-counting-using-YOLOV8-main\\\\backup_content'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "src_path = r\"C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\sort.py\"\n",
    "dest_path = r\"C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\backup_content\"\n",
    "\n",
    "# copy file from source to destination\n",
    "shutil.copy(src_path, dest_path)\n",
    "\n",
    "src_path = r\"C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\requirements.txt\"\n",
    "shutil.copy(src_path, dest_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wwx247S1H8tn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: filterpy in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from -r C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\requirements.txt (line 1)) (1.4.5)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from -r C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\requirements.txt (line 2)) (0.23.2)\n",
      "Collecting lap\n",
      "  Using cached lap-0.4.0.tar.gz (1.5 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: matplotlib in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from filterpy->-r C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\requirements.txt (line 1)) (3.7.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from filterpy->-r C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from filterpy->-r C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\requirements.txt (line 1)) (1.10.0)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from scikit-image->-r C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\requirements.txt (line 2)) (2.8.4)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from scikit-image->-r C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\requirements.txt (line 2)) (2024.5.22)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from scikit-image->-r C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\requirements.txt (line 2)) (0.4)\n",
      "Requirement already satisfied: pillow>=9.1 in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from scikit-image->-r C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\requirements.txt (line 2)) (10.3.0)\n",
      "Requirement already satisfied: imageio>=2.33 in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from scikit-image->-r C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\requirements.txt (line 2)) (2.34.1)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from scikit-image->-r C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\requirements.txt (line 2)) (22.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from matplotlib->filterpy->-r C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\requirements.txt (line 1)) (4.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from matplotlib->filterpy->-r C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from matplotlib->filterpy->-r C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\requirements.txt (line 1)) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from matplotlib->filterpy->-r C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\requirements.txt (line 1)) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from matplotlib->filterpy->-r C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\requirements.txt (line 1)) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from matplotlib->filterpy->-r C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\requirements.txt (line 1)) (1.0.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->filterpy->-r C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\requirements.txt (line 1)) (1.16.0)\n",
      "Building wheels for collected packages: lap\n",
      "  Building wheel for lap (setup.py): started\n",
      "  Building wheel for lap (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for lap\n",
      "Failed to build lap\n",
      "Installing collected packages: lap\n",
      "  Running setup.py install for lap: started\n",
      "  Running setup.py install for lap: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [39 lines of output]\n",
      "  Partial import of lap during the build process.\n",
      "  C:\\Users\\vinil\\AppData\\Local\\Temp\\pip-install-dfj9f9mo\\lap_c9f2655996854cd6ad06312830c810d7\\setup.py:223: DeprecationWarning:\n",
      "  \n",
      "    `numpy.distutils` is deprecated since NumPy 1.23.0, as a result\n",
      "    of the deprecation of `distutils` itself. It will be removed for\n",
      "    Python >= 3.12. For older Python versions it will remain present.\n",
      "    It is recommended to use `setuptools < 60.0` for those Python versions.\n",
      "    For more details, see:\n",
      "      https://numpy.org/devdocs/reference/distutils_status_migration.html\n",
      "  \n",
      "  \n",
      "    from numpy.distutils.core import setup\n",
      "  Generating cython files\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running config_cc\n",
      "  INFO: unifing config_cc, config, build_clib, build_ext, build commands --compiler options\n",
      "  running config_fc\n",
      "  INFO: unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options\n",
      "  running build_src\n",
      "  INFO: build_src\n",
      "  INFO: building extension \"lap._lapjv\" sources\n",
      "  INFO: building data_files sources\n",
      "  INFO: build_src: building npy-pkg config files\n",
      "  C:\\Users\\vinil\\anaconda3\\lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "    warnings.warn(\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-310\n",
      "  creating build\\lib.win-amd64-cpython-310\\lap\n",
      "  copying lap\\lapmod.py -> build\\lib.win-amd64-cpython-310\\lap\n",
      "  copying lap\\__init__.py -> build\\lib.win-amd64-cpython-310\\lap\n",
      "  running build_ext\n",
      "  INFO: No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils\n",
      "  INFO: customize MSVCCompiler\n",
      "  INFO: customize MSVCCompiler using build_ext\n",
      "  INFO: CCompilerOpt.cc_test_flags[1086] : testing flags (/O2)\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  INFO: CCompilerOpt.cache_flush[864] : write cache to path -> C:\\Users\\vinil\\AppData\\Local\\Temp\\pip-install-dfj9f9mo\\lap_c9f2655996854cd6ad06312830c810d7\\build\\temp.win-amd64-cpython-310\\Release\\ccompiler_opt_cache_ext.py\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for lap\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Running setup.py install for lap did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [39 lines of output]\n",
      "  Partial import of lap during the build process.\n",
      "  C:\\Users\\vinil\\AppData\\Local\\Temp\\pip-install-dfj9f9mo\\lap_c9f2655996854cd6ad06312830c810d7\\setup.py:223: DeprecationWarning:\n",
      "  \n",
      "    `numpy.distutils` is deprecated since NumPy 1.23.0, as a result\n",
      "    of the deprecation of `distutils` itself. It will be removed for\n",
      "    Python >= 3.12. For older Python versions it will remain present.\n",
      "    It is recommended to use `setuptools < 60.0` for those Python versions.\n",
      "    For more details, see:\n",
      "      https://numpy.org/devdocs/reference/distutils_status_migration.html\n",
      "  \n",
      "  \n",
      "    from numpy.distutils.core import setup\n",
      "  Generating cython files\n",
      "  running install\n",
      "  C:\\Users\\vinil\\anaconda3\\lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "    warnings.warn(\n",
      "  running build\n",
      "  running config_cc\n",
      "  INFO: unifing config_cc, config, build_clib, build_ext, build commands --compiler options\n",
      "  running config_fc\n",
      "  INFO: unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options\n",
      "  running build_src\n",
      "  INFO: build_src\n",
      "  INFO: building extension \"lap._lapjv\" sources\n",
      "  INFO: building data_files sources\n",
      "  INFO: build_src: building npy-pkg config files\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-310\n",
      "  creating build\\lib.win-amd64-cpython-310\\lap\n",
      "  copying lap\\lapmod.py -> build\\lib.win-amd64-cpython-310\\lap\n",
      "  copying lap\\__init__.py -> build\\lib.win-amd64-cpython-310\\lap\n",
      "  running build_ext\n",
      "  INFO: No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils\n",
      "  INFO: customize MSVCCompiler\n",
      "  INFO: customize MSVCCompiler using build_ext\n",
      "  INFO: CCompilerOpt.cc_test_flags[1086] : testing flags (/O2)\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  INFO: CCompilerOpt.cache_flush[864] : write cache to path -> C:\\Users\\vinil\\AppData\\Local\\Temp\\pip-install-dfj9f9mo\\lap_c9f2655996854cd6ad06312830c810d7\\build\\temp.win-amd64-cpython-310\\Release\\ccompiler_opt_cache_ext.py\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "Encountered error while trying to install package.\n",
      "\n",
      "lap\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r \"C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FDIvbG71Jkhn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\vinil\\appdata\\roaming\\python\\python310\\site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: filterpy in c:\\users\\vinil\\anaconda3\\lib\\site-packages (1.4.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from filterpy) (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from filterpy) (3.7.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from filterpy) (1.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from matplotlib->filterpy) (22.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from matplotlib->filterpy) (4.25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from matplotlib->filterpy) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from matplotlib->filterpy) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from matplotlib->filterpy) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from matplotlib->filterpy) (1.0.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from matplotlib->filterpy) (10.3.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from matplotlib->filterpy) (1.4.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vinil\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->filterpy) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Install the required packages\n",
    "!pip install opencv-python\n",
    "!pip install filterpy\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import imageio\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure the sort.py file is in the working directory\n",
    "sort_file_path = r\"C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\sort.py\"\n",
    "if not os.path.isfile(sort_file_path):\n",
    "    raise FileNotFoundError(f\"sort.py file not found at {sort_file_path}\")\n",
    "\n",
    "# Add the directory containing sort.py to the system path\n",
    "sys.path.append(os.path.dirname(sort_file_path))\n",
    "\n",
    "# Import sort module\n",
    "import sort\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hLCrvX5uxtAt",
    "outputId": "f1166d58-1669-4d2a-9bb2-958c1da18f28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.31  Python-3.10.9 torch-2.0.1+cpu CPU (AMD Ryzen 7 4800H with Radeon Graphics)\n",
      "Setup complete  (16 CPUs, 15.4 GB RAM, 269.9/475.3 GB disk)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user ultralytics\n",
    "\n",
    "%pip install ultralytics\n",
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RZfdp_TwpZJ"
   },
   "source": [
    "## Class definition for `YOLOv8_ObjectDetector` and `YOLOv8_ObjectCounter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FxwvNbWImzEG"
   },
   "outputs": [],
   "source": [
    "from sort import Sort\n",
    "\n",
    "class YOLOv8_ObjectDetector:\n",
    "    \"\"\"\n",
    "    A class for performing object detection on images and videos using YOLOv8.\n",
    "\n",
    "    Args:\n",
    "    ------------\n",
    "        model_file (str): Path to the YOLOv8 model file or yolo model variant name in ths format: [variant].pt\n",
    "        labels (list[str], optional): A list of class labels for the model. If None, uses the default labels from the model file.\n",
    "        classes (list[str], optional): Alias for labels. Deprecated.\n",
    "        conf (float, optional): Minimum confidence threshold for object detection.\n",
    "        iou (float, optional): Minimum IOU threshold for non-max suppression.\n",
    "\n",
    "    Attributes:\n",
    "    --------------\n",
    "        classes (list[str]): A list of class labels for the model ( a Dict is also acceptable).\n",
    "        conf (float): Minimum confidence threshold for object detection.\n",
    "        iou (float): Minimum IOU threshold for non-max suppression.\n",
    "        model (YOLO): The YOLOv8 model used for object detection.\n",
    "        model_name (str): The name of the YOLOv8 model file (without the .pt extension).\n",
    "\n",
    "    Methods :\n",
    "    -------------\n",
    "        default_display: Returns a default display (ultralytics plot implementation) of the object detection results.\n",
    "        custom_display: Returns a custom display of the object detection results.\n",
    "        predict_video: Predicts objects in a video and saves the results to a file.\n",
    "        predict_img: Predicts objects in an image and returns the detection results.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_file = 'yolov8n.pt', labels= None, classes = None, conf = 0.25, iou = 0.45 ):\n",
    "\n",
    "        self.classes = classes\n",
    "        self.conf = conf\n",
    "        self.iou = iou\n",
    "\n",
    "        self.model = YOLO(model_file)\n",
    "        self.model_name = model_file.split('.')[0]\n",
    "        self.results = None\n",
    "\n",
    "        if labels == None:\n",
    "            self.labels = self.model.names\n",
    "\n",
    "    def predict_img(self, img, verbose=True):\n",
    "        \"\"\"\n",
    "        Runs object detection on a single image.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            img (numpy.ndarray): Input image to perform object detection on.\n",
    "            verbose (bool): Whether to print detection details.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "            'ultralytics.yolo.engine.results.Results': A YOLO results object that contains \n",
    "             details about detection results :\n",
    "                    - Class IDs\n",
    "                    - Bounding Boxes\n",
    "                    - Confidence score\n",
    "                    ...\n",
    "        (pls refer to https://docs.ultralytics.com/reference/results/#results-api-reference for results API reference)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Run the model on the input image with the given parameters\n",
    "        results = self.model(img, classes=self.classes, conf=self.conf, iou=self.iou, verbose=verbose)\n",
    "\n",
    "        # Save the original image and the results for further analysis if needed\n",
    "        self.orig_img = img\n",
    "        self.results = results[0]\n",
    "\n",
    "        # Return the detection results\n",
    "        return results[0]\n",
    "\n",
    "\n",
    "\n",
    "    def default_display(self, show_conf=True, line_width=None, font_size=None, \n",
    "                        font='Arial.ttf', pil=False, example='abc'):\n",
    "        \"\"\"\n",
    "        Displays the detected objects on the original input image.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        show_conf : bool, optional\n",
    "            Whether to show the confidence score of each detected object, by default True.\n",
    "        line_width : int, optional\n",
    "            The thickness of the bounding box line in pixels, by default None.\n",
    "        font_size : int, optional\n",
    "            The font size of the text label for each detected object, by default None.\n",
    "        font : str, optional\n",
    "            The font type of the text label for each detected object, by default 'Arial.ttf'.\n",
    "        pil : bool, optional\n",
    "            Whether to return a PIL Image object, by default False.\n",
    "        example : str, optional\n",
    "            A string to display on the example bounding box, by default 'abc'.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray or PIL Image\n",
    "            The original input image with the detected objects displayed as bounding boxes.\n",
    "            If `pil=True`, a PIL Image object is returned instead.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If the input image has not been detected by calling the `predict_img()` method first.\n",
    "        \"\"\"\n",
    "        # Check if the `predict_img()` method has been called before displaying the detected objects\n",
    "        if self.results is None:\n",
    "            raise ValueError('No detected objects to display. Call predict_img() method first.')\n",
    "        \n",
    "        # Call the plot() method of the `self.results` object to display the detected objects on the original image\n",
    "        display_img = self.results.plot(show_conf, line_width, font_size, font, pil, example)\n",
    "        \n",
    "        # Return the displayed image\n",
    "        return display_img\n",
    "\n",
    "        \n",
    "\n",
    "    def custom_display(self, colors, show_cls = True, show_conf = True):\n",
    "        \"\"\"\n",
    "        Custom display method that draws bounding boxes and labels on the original image, \n",
    "        with additional options for showing class and confidence information.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        colors : list\n",
    "            A list of tuples specifying the color of each class.\n",
    "        show_cls : bool, optional\n",
    "            Whether to show class information in the label text. Default is True.\n",
    "        show_conf : bool, optional\n",
    "            Whether to show confidence information in the label text. Default is True.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        numpy.ndarray\n",
    "            The image with bounding boxes and labels drawn on it.\n",
    "        \"\"\"\n",
    "\n",
    "        img = self.orig_img\n",
    "        # calculate the bounding box thickness based on the image width and height\n",
    "        bbx_thickness = (img.shape[0] + img.shape[1]) // 450\n",
    "\n",
    "        for box in self.results.boxes:\n",
    "            textString = \"\"\n",
    "\n",
    "            # Extract object class and confidence score\n",
    "            score = box.conf.item() * 100\n",
    "            class_id = int(box.cls.item())\n",
    "\n",
    "            x1 , y1 , x2, y2 = np.squeeze(box.xyxy.numpy()).astype(int)\n",
    "\n",
    "            # Print detection info\n",
    "            if show_cls:\n",
    "                textString += f\"{self.labels[class_id]}\"\n",
    "\n",
    "            if show_conf:\n",
    "                textString += f\" {score:,.2f}%\"\n",
    "\n",
    "            # Calculate font scale based on object size\n",
    "            font = cv2.FONT_HERSHEY_COMPLEX\n",
    "            fontScale = (((x2 - x1) / img.shape[0]) + ((y2 - y1) / img.shape[1])) / 2 * 2.5\n",
    "            fontThickness = 1\n",
    "            textSize, baseline = cv2.getTextSize(textString, font, fontScale, fontThickness)\n",
    "\n",
    "            # Draw bounding box, a centroid and label on the image\n",
    "            img = cv2.rectangle(img, (x1,y1), (x2,y2), colors[class_id], bbx_thickness)\n",
    "            center_coordinates = ((x1 + x2)//2, (y1 + y2) // 2)\n",
    "\n",
    "            img =  cv2.circle(img, center_coordinates, 5 , (0,0,255), -1)\n",
    "            \n",
    "             # If there are no details to show on the image\n",
    "            if textString != \"\":\n",
    "                if (y1 < textSize[1]):\n",
    "                    y1 = y1 + textSize[1]\n",
    "                else:\n",
    "                    y1 -= 2\n",
    "                # show the details text in a filled rectangle\n",
    "                img = cv2.rectangle(img, (x1, y1), (x1 + textSize[0] , y1 -  textSize[1]), colors[class_id], cv2.FILLED)\n",
    "                img = cv2.putText(img, textString , \n",
    "                    (x1, y1), font, \n",
    "                    fontScale,  (0, 0, 0), fontThickness, cv2.LINE_AA)\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "    def predict_video(self, video_path, save_dir, save_format=\"avi\", display='custom', verbose=True, **display_args):\n",
    "        \"\"\"Runs object detection on each frame of a video and saves the output to a new video file.\n",
    "\n",
    "        Args:\n",
    "        ----------\n",
    "            video_path (str): The path to the input video file.\n",
    "            save_dir (str): The path to the directory where the output video file will be saved.\n",
    "            save_format (str, optional): The format for the output video file. Defaults to \"avi\".\n",
    "            display (str, optional): The type of display for the detection results. Defaults to 'custom'.\n",
    "            verbose (bool, optional): Whether to print information about the video file and output file. Defaults to True.\n",
    "            **display_args: Additional arguments to be passed to the display function.\n",
    "\n",
    "        Returns:\n",
    "        ------------\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Open the input video file\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        # Get the name of the input video file\n",
    "        vid_name = os.path.basename(video_path)\n",
    "\n",
    "        # Get the dimensions of each frame in the input video file\n",
    "        width = int(cap.get(3))  # get `width`\n",
    "        height = int(cap.get(4))  # get `height`\n",
    "\n",
    "        # Create the directory for the output video file if it does not already exist\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        # Set the name and path for the output video file\n",
    "        save_name = self.model_name + ' -- ' + vid_name.split('.')[0] + '.' + save_format\n",
    "        save_file = os.path.join(save_dir, save_name)\n",
    "\n",
    "        # Print information about the input and output video files if verbose is True\n",
    "        if verbose:\n",
    "            print(\"----------------------------\")\n",
    "            print(f\"DETECTING OBJECTS IN : {vid_name} : \")\n",
    "            print(f\"RESOLUTION : {width}x{height}\")\n",
    "            print('SAVING TO :' + save_file)\n",
    "\n",
    "        # Define an output VideoWriter object\n",
    "        out = cv2.VideoWriter(save_file,\n",
    "                              cv2.VideoWriter_fourcc(*\"MJPG\"),\n",
    "                              30, (width, height))\n",
    "\n",
    "        # Check if the input video file was opened correctly\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error opening video stream or file\")\n",
    "\n",
    "        # Read each frame of the input video file\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # If the frame was not read successfully, break the loop\n",
    "            if not ret:\n",
    "                print(\"Error reading frame\")\n",
    "                break\n",
    "\n",
    "            # Run object detection on the frame and calculate FPS\n",
    "            beg = time.time()\n",
    "            results = self.predict_img(frame, verbose=False)\n",
    "            if results is None:\n",
    "                print('***********************************************')\n",
    "            fps = 1 / (time.time() - beg)\n",
    "\n",
    "            # Display the detection results\n",
    "            if display == 'default':\n",
    "                frame = self.default_display(**display_args)\n",
    "            elif display == 'custom':\n",
    "                frame == self.custom_display(**display_args)\n",
    "\n",
    "            # Display the FPS on the frame\n",
    "            frame = cv2.putText(frame, f\"FPS : {fps:,.2f}\",\n",
    "                                (5, 15), cv2.FONT_HERSHEY_COMPLEX,\n",
    "                                0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "            # Write the frame to the output video file\n",
    "            out.write(frame)\n",
    "\n",
    "            # Exit the loop if the 'q' button is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        # After the loop release the cap and video writer\n",
    "        cap.release()\n",
    "        out.release()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qI-I-BBS81Fa"
   },
   "outputs": [],
   "source": [
    "class YOLOv8_ObjectCounter(YOLOv8_ObjectDetector):\n",
    "    \"\"\"\n",
    "    A class for counting objects in images or videos using the YOLOv8 Object Detection model.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    model_file : str\n",
    "        The filename of the YOLOv8 object detection model.\n",
    "    labels : list or None\n",
    "        The list of labels for the object detection model. If None, the labels will be loaded from the model file.\n",
    "    classes : list or None\n",
    "        The list of classes to detect. If None, all classes will be detected.\n",
    "    conf : float\n",
    "        The confidence threshold for object detection.\n",
    "    iou : float\n",
    "        The Intersection over Union (IoU) threshold for object detection.\n",
    "    track_max_age : int\n",
    "        The maximum age (in frames) of a track before it is deleted.\n",
    "    track_min_hits : int\n",
    "        The minimum number of hits required for a track to be considered valid.\n",
    "    track_iou_threshold : float\n",
    "        The IoU threshold for matching detections to existing tracks.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    predict_img(img, verbose=True)\n",
    "        Predicts objects in a single image and counts them.\n",
    "    predict_video(video_path, save_dir, save_format=\"avi\", display='custom', verbose=True, **display_args)\n",
    "        Predicts objects in a video and counts them.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_file = 'yolov8n.pt', labels= None, classes = None, conf = 0.25, iou = 0.45, \n",
    "                 track_max_age = 45, track_min_hits= 15, track_iou_threshold = 0.3 ):\n",
    "\n",
    "        super().__init__(model_file , labels, classes, conf, iou)\n",
    "\n",
    "        self.track_max_age = track_max_age\n",
    "        self.track_min_hits = track_min_hits\n",
    "        self.track_iou_threshold = track_iou_threshold\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    def predict_video(self, video_path, save_dir, save_format = \"avi\", \n",
    "                      display = 'custom', verbose = True, **display_args):\n",
    "        \n",
    "        \"\"\"\n",
    "    Runs object detection on a video file and saves the output as a new video file.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the input video file.\n",
    "        save_dir (str): Path to the directory where the output video file will be saved.\n",
    "        save_format (str, optional): Format of the output video file. Defaults to \"avi\".\n",
    "        display (str, optional): Type of display to use for object detection results. Options are \"default\" or \"custom\". \n",
    "                                Defaults to \"custom\".\n",
    "        verbose (bool, optional): If True, prints information about the input and output video files. Defaults to True.\n",
    "        **display_args (dict, optional): Additional arguments to pass to the display function. \n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        # Get video name \n",
    "        vid_name = os.path.basename(video_path)\n",
    "\n",
    "\n",
    "        # Get frame dimensions and print information about input video file\n",
    "        width  = int(cap.get(3) )  # get `width` \n",
    "        height = int(cap.get(4) )  # get `height` \n",
    "\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        save_name = self.model_name + ' -- ' + vid_name.split('.')[0] + '.' + save_format\n",
    "        save_file = os.path.join(save_dir, save_name)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"----------------------------\")\n",
    "            print(f\"DETECTING OBJECTS IN : {vid_name} : \")\n",
    "            print(f\"RESOLUTION : {width}x{height}\")\n",
    "            print('SAVING TO :' + save_file)\n",
    "\n",
    "        # define an output VideoWriter  object\n",
    "        out = cv2.VideoWriter(save_file,\n",
    "                            cv2.VideoWriter_fourcc(*\"MJPG\"),\n",
    "                            30,(width,height))\n",
    "\n",
    "        # Check if the video is opened correctly\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error opening video stream or file\")\n",
    "\n",
    "        # Initialize object tracker\n",
    "        tracker = sort.Sort(max_age = self.track_max_age, min_hits= self.track_min_hits , \n",
    "                            iou_threshold = self.track_iou_threshold)\n",
    "        \n",
    "        # Initialize variables for object counting\n",
    "        totalCount = []\n",
    "        currentArray = np.empty((0, 5))\n",
    "\n",
    "\n",
    "        # Read the video frames\n",
    "        while cap.isOpened():\n",
    "\n",
    "            detections = np.empty((0, 5))\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # If the frame was not read successfully, break the loop\n",
    "            if not ret:\n",
    "                print(\"Error reading frame\")\n",
    "                break\n",
    "\n",
    "            # Run object detection on the frame and calculate FPS\n",
    "            beg = time.time()\n",
    "            results = self.predict_img(frame, verbose = False)\n",
    "            if results == None:\n",
    "                print('***********************************************')\n",
    "            fps = 1 / (time.time() - beg)\n",
    "            for box in results.boxes:\n",
    "                score = box.conf.item() * 100\n",
    "                class_id = int(box.cls.item())\n",
    "\n",
    "                x1 , y1 , x2, y2 = np.squeeze(box.xyxy.numpy()).astype(int)\n",
    "\n",
    "                currentArray = np.array([x1, y1, x2, y2, score])\n",
    "                detections = np.vstack((detections, currentArray))\n",
    "\n",
    "            # Update object tracker \n",
    "            resultsTracker = tracker.update(detections)\n",
    "            for result in resultsTracker:\n",
    "                #print(type(result))\n",
    "\n",
    "                # Get the tracker results\n",
    "                x1, y1, x2, y2, id = result\n",
    "                x1, y1, x2, y2, id = int(x1), int(y1), int(x2), int(y2), int(id)\n",
    "                #print(result)\n",
    "\n",
    "                # Display current objects IDs\n",
    "                w, h = x2 - x1, y2 - y1\n",
    "                cx, cy = x1 + w // 2, y1 + h // 2\n",
    "                id_txt = f\"ID: {str(id)}\"\n",
    "                cv2.putText(frame, id_txt, (cx, cy), 4, 0.5, (0, 0, 255), 1)\n",
    "\n",
    "                # if we haven't seen aprticular object ID before, register it in a list \n",
    "                if totalCount.count(id) == 0:\n",
    "                    totalCount.append(id)\n",
    "\n",
    "            # Display detection results\n",
    "            if display == 'default':\n",
    "                frame = self.default_display(**display_args)\n",
    "            \n",
    "            elif display == 'custom':\n",
    "                frame == self.custom_display(**display_args)\n",
    "\n",
    "            # Display FPS on frame\n",
    "            frame = cv2.putText(frame,f\"FPS : {fps:,.2f}\" , \n",
    "                                (5,55), cv2.FONT_HERSHEY_COMPLEX, \n",
    "                            0.5,  (0,255,255), 1, cv2.LINE_AA)\n",
    "            \n",
    "            # Display Counting results\n",
    "            count_txt = f\"TOTAL COUNT : {len(totalCount)}\"\n",
    "            frame = cv2.putText(frame, count_txt, (5,45), cv2.FONT_HERSHEY_COMPLEX, 2, (0, 0, 255), 2)\n",
    "        \n",
    "\n",
    "            # append frame to the video file\n",
    "            out.write(frame)\n",
    "            \n",
    "            # the 'q' button is set as the\n",
    "            # quitting button you may use any\n",
    "            # desired button of your choice\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        # After the loop release the cap \n",
    "        cap.release()\n",
    "        out.release()\n",
    "        print(len(totalCount))\n",
    "        print(totalCount)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVPnI_ZdruJr"
   },
   "source": [
    "### preparing file paths and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Y-xXMgUjiX_0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "vid_results_path = r'C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\results_video'\n",
    "test_vids_path = r'C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\test vids'\n",
    "\n",
    "\n",
    "if not os.path.isdir(vid_results_path):\n",
    "    os.makedirs(vid_results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfZjVGJSr6ie"
   },
   "source": [
    "### Instanciating YOLOv8_ObjectCounter objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YhaMvVPHAy1r"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "yolo_names = ['yolov8n.pt', 'yolov8m.pt', 'yolov8s.pt',  'yolov8l.pt']\n",
    "colors = []\n",
    "for _ in range(80):\n",
    "    rand_tuple = (random.randint(50, 255), random.randint(50, 255), random.randint(50, 255))\n",
    "    colors.append(rand_tuple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214,
     "referenced_widgets": [
      "74c63422783a4b9393937c147ab52312",
      "bdf76418bd444bda805e6bc72c20b41d",
      "c23dbe0f8ae842ac86431dcf6df4e30e",
      "589f98818ece45c08e8b08aba2e10a52",
      "cdd6765e1e8c4d9290b925723668ce4d",
      "fd8b2392e0724dbca38117ea7e3d8092",
      "b7d130944252418cb01436e472e99045",
      "9fe5f9418c6b4ba59f9cfa672418bdc1",
      "275c1550ed0c4ebfb10623e0cb831f40",
      "364379aa31b8436995eff048997e1889",
      "f3cb7f2d8cab439aaf1be6627d896861",
      "d63e0f6aab9c4f39a30077a535aa2fdb",
      "283fcfb3503f4ac6af092bcbc65539eb",
      "a06e3aeca38a4a40a38c4100c3b99a29",
      "440d6d451ea149c0a88032241b62e89e",
      "cff8848a7ca448eab31e86871e0d965a",
      "9d203e13b0c340cb83d10cd019b24834",
      "8b6fed3895aa41cdafa60d9247406960",
      "b861e108e0fb44f299d6ee4bf4f31716",
      "cfb4327e8b524deab10fdf36251279d4",
      "5a631dc99881412c9248457c56339e51",
      "94ff81cffe4d437e99cb324f03434222",
      "f13a88dff2da4a009813c56a05da6c47",
      "19b7ecc1a0394234bae9059916ba5f2c",
      "296e08bae8b14d03b6ba6aecbf54c0e1",
      "acc1ea899bc747478bd68d716511bfcc",
      "b0e0fc63c332428686df86928b393d4d",
      "ae75f9cc781541d6864e4c534f0f934d",
      "c203344233064ec2a22bb8ca52475952",
      "3af23971bbbb47788282c06fa9567059",
      "8f779fc5df5949c1b78adf7b80af68cb",
      "0e9f5322c2534d9fb0e62a3d4064b9dc",
      "3b75806d3422402e88fe4a6a34ac0c13",
      "d60b6159cd94445193dcd3c710944f0b",
      "51a52a8f1f9a437091a15d8e4698ba0a",
      "fdd0228e512f4cd1a2437ccb832e784c",
      "740c00fec1534ba39dae12b4c21ba4bc",
      "0c6b9533bf5940c88d407d6ab4c2d3ac",
      "57786ef5757b43a08728e78292f3f364",
      "6137f253f1a948aa988e0f7610d4c6f1",
      "c21f8ec0668041008a7ee7b4243147d9",
      "d8c7c0efc98f435ab4d6472f0b503d22",
      "e01152f2c2c5454d8e03da07a2de2b0b",
      "8b38405fd384473cabaabec6d7aebfcf"
     ]
    },
    "id": "E7O7yl3A3kxE",
    "outputId": "3ca4fbb5-3211-40c7-90cb-779a6bb055d8"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "counters = []\n",
    "for yolo_name in yolo_names:\n",
    "    counter = YOLOv8_ObjectCounter(yolo_name, conf = 0.60 )\n",
    "    counters.append(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLYwvejewThL"
   },
   "source": [
    "### Performing object detection, tracking and counting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ph04-AKe3z9x",
    "outputId": "af509b99-84b9-47e2-9241-22f6d256c592"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "DETECTING OBJECTS IN : traffic 2.mp4 : \n",
      "RESOLUTION : 1280x720\n",
      "SAVING TO :C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\results_video\\yolov8n -- traffic 2.avi\n",
      "Error reading frame\n",
      "26\n",
      "[8, 7, 6, 5, 4, 3, 2, 1, 9, 10, 12, 11, 13, 15, 17, 18, 19, 21, 22, 26, 27, 25, 29, 30, 32, 33]\n",
      "----------------------------\n",
      "DETECTING OBJECTS IN : traffic 2.mp4 : \n",
      "RESOLUTION : 1280x720\n",
      "SAVING TO :C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\results_video\\yolov8m -- traffic 2.avi\n",
      "Error reading frame\n",
      "25\n",
      "[43, 42, 41, 40, 39, 38, 44, 45, 46, 47, 48, 49, 55, 56, 58, 61, 60, 62, 57, 66, 69, 64, 70, 73, 74]\n",
      "----------------------------\n",
      "DETECTING OBJECTS IN : traffic 2.mp4 : \n",
      "RESOLUTION : 1280x720\n",
      "SAVING TO :C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\results_video\\yolov8s -- traffic 2.avi\n",
      "Error reading frame\n",
      "20\n",
      "[85, 84, 83, 82, 81, 80, 86, 89, 87, 90, 96, 99, 103, 105, 108, 112, 111, 113, 118, 119]\n",
      "----------------------------\n",
      "DETECTING OBJECTS IN : traffic 2.mp4 : \n",
      "RESOLUTION : 1280x720\n",
      "SAVING TO :C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\results_video\\yolov8l -- traffic 2.avi\n",
      "Error reading frame\n",
      "29\n",
      "[130, 129, 128, 127, 126, 125, 124, 123, 131, 132, 133, 135, 136, 141, 144, 145, 142, 147, 146, 149, 150, 151, 157, 159, 160, 156, 164, 167, 166]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "for counter in counters:\n",
    "    counter.predict_video(video_path= r'C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main/test vids/traffic 2.mp4'\n",
    ", save_dir = vid_results_path, save_format = \"avi\", display = 'custom', colors = colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fCGL2kUwh7f"
   },
   "source": [
    "## Preparing results for download "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E3GYF0xsxd4h",
    "outputId": "69aba09c-2538-4981-e016-8abca982341b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip file 'vid_results.zip' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Define the directory you want to zip\n",
    "directory_to_zip = r\"C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\results_video\"\n",
    "\n",
    "# Define the name of the zip file to create\n",
    "zip_file_name = \"vid_results.zip\"\n",
    "\n",
    "# Initialize ZipFile object\n",
    "with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n",
    "    # Walk through each file in the directory\n",
    "    for foldername, subfolders, filenames in os.walk(directory_to_zip):\n",
    "        for filename in filenames:\n",
    "            # Create complete filepath of file in directory\n",
    "            file_path = os.path.join(foldername, filename)\n",
    "            # Add file to zip\n",
    "            zipf.write(file_path, os.path.relpath(file_path, directory_to_zip))\n",
    "\n",
    "print(f\"Zip file '{zip_file_name}' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the checkpoint:\n",
      "epoch\n",
      "best_fitness\n",
      "model\n",
      "ema\n",
      "updates\n",
      "optimizer\n",
      "train_args\n",
      "date\n",
      "version\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DetectionModel' object has no attribute 'fc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Load YOLOv8 model from .pt file\u001b[39;00m\n\u001b[0;32m     76\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mvinil\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mObject-tracking-and-counting-using-YOLOV8-main\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124myolov8n.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 77\u001b[0m yolov8_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_yolov8_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Directory containing test videos\u001b[39;00m\n\u001b[0;32m     80\u001b[0m test_videos_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mvinil\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mObject-tracking-and-counting-using-YOLOV8-main\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtest vids\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[8], line 21\u001b[0m, in \u001b[0;36mload_yolov8_model\u001b[1;34m(model_path)\u001b[0m\n\u001b[0;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Assuming 'model' contains the model itself\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Example adjustment: Replace the final fully connected layer for your specific output\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m num_ftrs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[38;5;241m.\u001b[39min_features\n\u001b[0;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(num_ftrs, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Replace with your output layer\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Set model to evaluation mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DetectionModel' object has no attribute 'fc'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Function to load YOLOv8 model\n",
    "def load_yolov8_model(model_path):\n",
    "    # Load the model checkpoint\n",
    "    checkpoint = torch.load(model_path)\n",
    "    \n",
    "    # Print out the keys in the checkpoint\n",
    "    print(\"Keys in the checkpoint:\")\n",
    "    for key in checkpoint.keys():\n",
    "        print(key)\n",
    "    \n",
    "    # Adjust the loading logic based on the actual keys in your checkpoint\n",
    "    model = checkpoint['model']  # Assuming 'model' contains the model itself\n",
    "    \n",
    "    # Example adjustment: Replace the final fully connected layer for your specific output\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 2)  # Replace with your output layer\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Function to perform inference on a video\n",
    "def perform_inference(video_path, model):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file {video_path}\")\n",
    "        return\n",
    "    \n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Preprocess frame (if needed) and convert to tensor\n",
    "        # Example: convert BGR to RGB and normalize\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        rgb_frame = rgb_frame / 255.0  # Normalize to [0, 1]\n",
    "        input_tensor = torch.from_numpy(rgb_frame.transpose((2, 0, 1))).unsqueeze(0).float()\n",
    "        \n",
    "        # Perform object detection using your model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tensor)\n",
    "        \n",
    "        # Example placeholder: process outputs and visualize detections\n",
    "        # Replace with your actual object detection and visualization code\n",
    "        # Example: draw bounding boxes on frame\n",
    "        # for box in detected_boxes:\n",
    "        #     cv2.rectangle(frame, box, color=(0, 255, 0), thickness=2)\n",
    "        \n",
    "        # Display or save processed frame (as needed)\n",
    "        cv2.imshow('Inference Output', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example of evaluating on a test dataset\n",
    "def evaluate_on_test_dataset(model, test_videos_dir):\n",
    "    for video_file in os.listdir(test_videos_dir):\n",
    "        video_path = os.path.join(test_videos_dir, video_file)\n",
    "        print(f\"Evaluating video: {video_path}\")\n",
    "        perform_inference(video_path, model)\n",
    "\n",
    "# Load YOLOv8 model from .pt file\n",
    "model_path = r'C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\yolov8n.pt'\n",
    "yolov8_model = load_yolov8_model(model_path)\n",
    "\n",
    "# Directory containing test videos\n",
    "test_videos_dir = r'C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\test vids'\n",
    "\n",
    "# Evaluate on the test dataset\n",
    "evaluate_on_test_dataset(yolov8_model, test_videos_dir)\n",
    "\n",
    "# Calculate evaluation metrics (example placeholder)\n",
    "precision = 0.75\n",
    "recall = 0.80\n",
    "f1_score = 0.77\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\vinil\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#feature fusion\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class YOLOv8:\n",
    "    def __init__(self, config_path, weights_path, class_names_path):\n",
    "        self.net = self.load_model(config_path, weights_path)\n",
    "        self.classes = self.load_classes(class_names_path)\n",
    "        self.colors = np.random.uniform(0, 255, size=(len(self.classes), 3))\n",
    "\n",
    "    def load_model(self, config_path, weights_path):\n",
    "        # Load YOLO model\n",
    "        net = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n",
    "        return net\n",
    "\n",
    "    def load_classes(self, class_names_path):\n",
    "        # Load class names\n",
    "        with open(class_names_path, 'r') as f:\n",
    "            classes = f.read().strip().split('\\n')\n",
    "        return classes\n",
    "\n",
    "    def detect_objects(self, image):\n",
    "        # Preprocess the image\n",
    "        blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "        self.net.setInput(blob)\n",
    "\n",
    "        # Get the output layer names\n",
    "        ln = self.net.getLayerNames()\n",
    "        ln = [ln[i - 1] for i in self.net.getUnconnectedOutLayers()]\n",
    "\n",
    "        # Run the forward pass\n",
    "        layer_outputs = self.net.forward(ln)\n",
    "\n",
    "        # Feature fusion logic\n",
    "        fused_features = self.feature_fusion(layer_outputs)\n",
    "\n",
    "        boxes = []\n",
    "        confidences = []\n",
    "        class_ids = []\n",
    "\n",
    "        for output in fused_features:\n",
    "            for detection in output:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "\n",
    "                if confidence > 0.5:\n",
    "                    box = detection[0:4] * np.array([image.shape[1], image.shape[0], image.shape[1], image.shape[0]])\n",
    "                    (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "                    x = int(centerX - (width / 2))\n",
    "                    y = int(centerY - (height / 2))\n",
    "\n",
    "                    boxes.append([x, y, int(width), int(height)])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "        idxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "        result = []\n",
    "        if len(idxs) > 0:\n",
    "            for i in idxs.flatten():\n",
    "                result.append((boxes[i], confidences[i], class_ids[i]))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def feature_fusion(self, layer_outputs):\n",
    "        # Example feature fusion logic: simple concatenation\n",
    "        fused_features = []\n",
    "        for output in layer_outputs:\n",
    "            fused_features.append(output)\n",
    "        return fused_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FPN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(FPN, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # Lateral layers\n",
    "        self.lateral3 = nn.Conv2d(in_channels[0], out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.lateral4 = nn.Conv2d(in_channels[1], out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.lateral5 = nn.Conv2d(in_channels[2], out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        # Smooth layers\n",
    "        self.smooth3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.smooth4 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.smooth5 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, c3, c4, c5):\n",
    "        # Lateral connections\n",
    "        p5 = self.lateral5(c5)\n",
    "        p4 = self.lateral4(c4) + F.interpolate(p5, scale_factor=2, mode='nearest')\n",
    "        p3 = self.lateral3(c3) + F.interpolate(p4, scale_factor=2, mode='nearest')\n",
    "\n",
    "        # Smooth layers\n",
    "        p3 = self.smooth3(p3)\n",
    "        p4 = self.smooth4(p4)\n",
    "        p5 = self.smooth5(p5)\n",
    "\n",
    "        return p3, p4, p5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with the original YOLOv8 model loading\n",
    "# self.model = YOLO(model_file)\n",
    "self.model = ModifiedYOLOv8(YOLO(model_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Define the FPN Module\n",
    "class FPN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(FPN, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # Lateral layers\n",
    "        self.lateral3 = nn.Conv2d(in_channels[0], out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.lateral4 = nn.Conv2d(in_channels[1], out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.lateral5 = nn.Conv2d(in_channels[2], out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        # Smooth layers\n",
    "        self.smooth3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.smooth4 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.smooth5 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, c3, c4, c5):\n",
    "        # Lateral connections\n",
    "        p5 = self.lateral5(c5)\n",
    "        p4 = self.lateral4(c4) + F.interpolate(p5, scale_factor=2, mode='nearest')\n",
    "        p3 = self.lateral3(c3) + F.interpolate(p4, scale_factor=2, mode='nearest')\n",
    "\n",
    "        # Smooth layers\n",
    "        p3 = self.smooth3(p3)\n",
    "        p4 = self.smooth4(p4)\n",
    "        p5 = self.smooth5(p5)\n",
    "\n",
    "        return p3, p4, p5\n",
    "\n",
    "# Integrate FPN with YOLOv8\n",
    "class ModifiedYOLOv8(nn.Module):\n",
    "    def __init__(self, original_yolo_model):\n",
    "        super(ModifiedYOLOv8, self).__init__()\n",
    "        self.original_yolo_model = original_yolo_model\n",
    "\n",
    "        # Define the FPN\n",
    "        self.fpn = FPN(in_channels=[256, 512, 1024], out_channels=256)  # Adjust in_channels as needed\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features using the original YOLOv8 backbone\n",
    "        features = self.original_yolo_model.backbone(x)\n",
    "        c3, c4, c5 = features[-3], features[-2], features[-1]\n",
    "\n",
    "        # Apply FPN\n",
    "        p3, p4, p5 = self.fpn(c3, c4, c5)\n",
    "\n",
    "        # Continue with the original YOLOv8 head (detection part)\n",
    "        detections = self.original_yolo_model.head([p3, p4, p5])\n",
    "\n",
    "        return detections\n",
    "\n",
    "# Load the original YOLOv8 model\n",
    "original_yolo_model =  r'C:\\Users\\vinil\\Downloads\\Object-tracking-and-counting-using-YOLOV8-main\\ylovm.pt'\n",
    "\n",
    "# Create the modified YOLOv8 model with FPN\n",
    "modified_yolo_model = ModifiedYOLOv8(original_yolo_model)\n",
    "\n",
    "class YOLOv8_ObjectDetector:\n",
    "    \"\"\"\n",
    "    A class for performing object detection on images and videos using YOLOv8.\n",
    "\n",
    "    Args:\n",
    "    ------------\n",
    "        model_file (str): Path to the YOLOv8 model file or yolo model variant name in the format: [variant].pt\n",
    "        labels (list[str], optional): A list of class labels for the model. If None, uses the default labels from the model file.\n",
    "        classes (list[str], optional): Alias for labels. Deprecated.\n",
    "        conf (float, optional): Minimum confidence threshold for object detection.\n",
    "        iou (float, optional): Minimum IOU threshold for object detection.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_file='yolov8n.pt', labels=None, classes=None, conf=0.25, iou=0.45):\n",
    "        self.model_file = model_file\n",
    "        self.labels = labels\n",
    "        self.classes = classes\n",
    "        self.conf = conf\n",
    "        self.iou = iou\n",
    "\n",
    "        # Load the original YOLOv8 model\n",
    "        original_yolo_model = YOLO(model_file)\n",
    "\n",
    "        # Create the modified YOLOv8 model with FPN\n",
    "        self.model = ModifiedYOLOv8(original_yolo_model)\n",
    "        self.model_name = model_file.split('.')[0]\n",
    "        self.results = None\n",
    "\n",
    "        if labels is None:\n",
    "            self.labels = self.model.original_yolo_model.names\n",
    "\n",
    "    def predict_img(self, img, verbose=True):\n",
    "        \"\"\"\n",
    "        Runs object detection on a single image.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            img (numpy.ndarray): Input image to perform object detection on.\n",
    "            verbose (bool): Whether to print detection details.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "            'ultralytics.yolo.engine.results.Results': A YOLO results object that contains \n",
    "             details about detection results :\n",
    "                    - Class IDs\n",
    "                    - Bounding Boxes\n",
    "                    - Confidence score\n",
    "                    ...\n",
    "        (pls refer to https://docs.ultralytics.com/reference/results/#results-api-reference for results API reference)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Run the model on the input image with the given parameters\n",
    "        results = self.model(img, classes=self.classes, conf=self.conf, iou=self.iou, verbose=verbose)\n",
    "\n",
    "        # Save the original image and the results for further analysis if needed\n",
    "        self.orig_img = img\n",
    "        self.results = results[0]\n",
    "\n",
    "        # Return the detection results\n",
    "        return results[0]\n",
    "\n",
    "    def default_display(self, show_conf=True, line_width=None, font_size=None, \n",
    "                        font='Arial.ttf', pil=False, example='abc'):\n",
    "        \"\"\"\n",
    "        Displays the detected objects on the original input image.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        show_conf : bool, optional\n",
    "            Whether to show the confidence score of each detected object, by default True.\n",
    "        line_width : int, optional\n",
    "            The thickness of the bounding box line in pixels, by default None.\n",
    "        font_size : int, optional\n",
    "            The font size of the text label for each detected object, by default None.\n",
    "        font : str, optional\n",
    "            The font type of the text label for each detected object, by default 'Arial.ttf'.\n",
    "        pil : bool, optional\n",
    "            Whether to return a PIL Image object, by default False.\n",
    "        example : str, optional\n",
    "            A string to display on the example bounding box, by default 'abc'.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray or PIL Image\n",
    "            The original input image with the detected objects displayed as bounding boxes.\n",
    "            If `pil=True`, a PIL Image object is returned instead.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If the input image has not been detected by calling the `predict_img()` method first.\n",
    "        \"\"\"\n",
    "        # Check if the `predict_img()` method has been called before displaying the detected objects\n",
    "        if self.results is None:\n",
    "            \n",
    "            raise ValueError('No detected objects to display. Call predict_img() method first.')\n",
    "        \n",
    "        # Call the plot() method of the `self.results` object to display the detected objects on the original image\n",
    "        display_img = self.results.plot(show_conf, line_width, font_size, font, pil, example)\n",
    "        \n",
    "        # Return the displayed image\n",
    "        return display_img\n",
    "\n",
    "    def custom_display(self, colors, show_cls=True, show_conf=True):\n",
    "        \"\"\"\n",
    "        Custom display method that draws bounding boxes and labels on the original image, \n",
    "        with additional options for showing class and confidence information.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        colors : list\n",
    "            A list of tuples specifying the color of each class.\n",
    "        show_cls : bool, optional\n",
    "            Whether to show class information in the label text. Default is True.\n",
    "        show_conf : bool, optional\n",
    "            Whether to show confidence information in the label text. Default is True.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        numpy.ndarray\n",
    "            The image with bounding boxes and labels drawn on it.\n",
    "        \"\"\"\n",
    "\n",
    "        img = self.orig_img\n",
    "        # calculate the bounding box thickness based on the image width and height\n",
    "        bbx_thickness = (img.shape[0] + img.shape[1]) // 450\n",
    "\n",
    "        for box in self.results.boxes:\n",
    "            textString = \"\"\n",
    "\n",
    "            # Extract object class and confidence score\n",
    "            score = box.conf.item() * 100\n",
    "            class_id = int(box.cls.item())\n",
    "\n",
    "            x1, y1, x2, y2 = np.squeeze(box.xyxy.numpy()).astype(int)\n",
    "\n",
    "            # Print detection info\n",
    "            if show_cls:\n",
    "                textString += f\"{self.labels[class_id]}\"\n",
    "\n",
    "            if show_conf:\n",
    "                textString += f\" {score:,.2f}%\"\n",
    "\n",
    "            # Calculate font scale based on object size\n",
    "            font = cv2.FONT_HERSHEY_COMPLEX\n",
    "            fontScale = (((x2 - x1) / img.shape[0]) + ((y2 - y1) / img.shape[1])) / 2 * 2.5\n",
    "            fontThickness = 1\n",
    "            textSize, baseline = cv2.getTextSize(textString, font, fontScale, fontThickness)\n",
    "\n",
    "            # Draw bounding box, a centroid and label on the image\n",
    "            img = cv2.rectangle(img, (x1, y1), (x2, y2), colors[class_id], bbx_thickness)\n",
    "            center_coordinates = ((x1 + x2) // 2, (y1 + y2) // 2)\n",
    "            img = cv2.circle(img, center_coordinates, 5, (0, 0, 255), -1)\n",
    "\n",
    "            # If there are no details to show on the image\n",
    "            if textString != \"\":\n",
    "                if y1 < textSize[1]:\n",
    "                    y1 = y1 + textSize[1]\n",
    "                else:\n",
    "                    y1 -= 2\n",
    "                # show the details text in a filled rectangle\n",
    "                img = cv2.rectangle(img, (x1, y1), (x1 + textSize[0], y1 - textSize[1]), colors[class_id], cv2.FILLED)\n",
    "                img = cv2.putText(img, textString,\n",
    "                                  (x1, y1), font,\n",
    "                                  fontScale, (0, 0, 0), fontThickness, cv2.LINE_AA)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def predict_video(self, video_path, save_dir, save_format=\"avi\", display='custom', verbose=True, **display_args):\n",
    "        \"\"\"\n",
    "        Runs object detection on each frame of a video and saves the output to a new video file.\n",
    "\n",
    "        Args:\n",
    "        ----------\n",
    "            video_path (str): The path to the input video file.\n",
    "            save_dir (str): The path to the directory where the output video file will be saved.\n",
    "            save_format (str, optional): The format for the output video file. Defaults to \"avi\".\n",
    "            display (str, optional): The type of display for the detection results. Defaults to 'custom'.\n",
    "            verbose (bool, optional): Whether to print information about the video file and output file. Defaults to True.\n",
    "            **display_args: Additional arguments to be passed to the display function.\n",
    "\n",
    "        Returns:\n",
    "        ------------\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Open the input video file\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        # Get the name of the input video file\n",
    "        vid_name = os.path.basename(video_path)\n",
    "\n",
    "        # Get the dimensions of each frame in the input video file\n",
    "        width = int(cap.get(3))  # get `width`\n",
    "        height = int(cap.get(4))  # get `height`\n",
    "\n",
    "        # Create the directory for the output video file if it does not already exist\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        # Set the name and path for the output video file\n",
    "        save_name = self.model_name + ' -- ' + vid_name.split('.')[0] + '.' + save_format\n",
    "        save_file = os.path.join(save_dir, save_name)\n",
    "\n",
    "        # Print information about the input and output video files if verbose is True\n",
    "        if verbose:\n",
    "            print(\"----------------------------\")\n",
    "            print(f\"DETECTING OBJECTS IN: {vid_name}:\")\n",
    "            print(f\"RESOLUTION: {width}x{height}\")\n",
    "            print('SAVING TO:' + save_file)\n",
    "\n",
    "        # Define an output VideoWriter object\n",
    "        out = cv2.VideoWriter(save_file,\n",
    "                              cv2.VideoWriter_fourcc(*\"MJPG\"),\n",
    "                              30, (width, height))\n",
    "\n",
    "        # Check if the input video file was opened correctly\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error opening video stream or file\")\n",
    "\n",
    "        # Read each frame of the input video file\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # If the frame was not read successfully, break the loop\n",
    "            if not ret:\n",
    "                print(\"Error reading frame\")\n",
    "                break\n",
    "\n",
    "            # Run object detection on the frame and calculate FPS\n",
    "            beg = time.time()\n",
    "            results = self.predict_img(frame, verbose=False)\n",
    "            if results is None:\n",
    "                print('***********************************************')\n",
    "            fps = 1 / (time.time() - beg)\n",
    "\n",
    "            # Display the detection results\n",
    "            if display == 'default':\n",
    "                frame = self.default_display(**display_args)\n",
    "            elif display == 'custom':\n",
    "                frame = self.custom_display(**display_args)\n",
    "\n",
    "            # Display the FPS on the frame\n",
    "            frame = cv2.putText(frame, f\"FPS: {fps:,.2f}\",\n",
    "                                (5, 15), cv2.FONT_HERSHEY_COMPLEX,\n",
    "                                0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "            # Write the frame to the output video file\n",
    "            out.write(frame)\n",
    "\n",
    "            # Exit the loop if the 'q' button is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        # After the loop release the cap and video writer\n",
    "        cap.release()\n",
    "        out.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c6b9533bf5940c88d407d6ab4c2d3ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e9f5322c2534d9fb0e62a3d4064b9dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19b7ecc1a0394234bae9059916ba5f2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae75f9cc781541d6864e4c534f0f934d",
      "placeholder": "​",
      "style": "IPY_MODEL_c203344233064ec2a22bb8ca52475952",
      "value": "100%"
     }
    },
    "275c1550ed0c4ebfb10623e0cb831f40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "283fcfb3503f4ac6af092bcbc65539eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9d203e13b0c340cb83d10cd019b24834",
      "placeholder": "​",
      "style": "IPY_MODEL_8b6fed3895aa41cdafa60d9247406960",
      "value": "100%"
     }
    },
    "296e08bae8b14d03b6ba6aecbf54c0e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3af23971bbbb47788282c06fa9567059",
      "max": 22573363,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8f779fc5df5949c1b78adf7b80af68cb",
      "value": 22573363
     }
    },
    "364379aa31b8436995eff048997e1889": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3af23971bbbb47788282c06fa9567059": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b75806d3422402e88fe4a6a34ac0c13": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "440d6d451ea149c0a88032241b62e89e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a631dc99881412c9248457c56339e51",
      "placeholder": "​",
      "style": "IPY_MODEL_94ff81cffe4d437e99cb324f03434222",
      "value": " 49.7M/49.7M [00:00&lt;00:00, 144MB/s]"
     }
    },
    "51a52a8f1f9a437091a15d8e4698ba0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_57786ef5757b43a08728e78292f3f364",
      "placeholder": "​",
      "style": "IPY_MODEL_6137f253f1a948aa988e0f7610d4c6f1",
      "value": "100%"
     }
    },
    "57786ef5757b43a08728e78292f3f364": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "589f98818ece45c08e8b08aba2e10a52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_364379aa31b8436995eff048997e1889",
      "placeholder": "​",
      "style": "IPY_MODEL_f3cb7f2d8cab439aaf1be6627d896861",
      "value": " 6.23M/6.23M [00:00&lt;00:00, 93.1MB/s]"
     }
    },
    "5a631dc99881412c9248457c56339e51": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6137f253f1a948aa988e0f7610d4c6f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "740c00fec1534ba39dae12b4c21ba4bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e01152f2c2c5454d8e03da07a2de2b0b",
      "placeholder": "​",
      "style": "IPY_MODEL_8b38405fd384473cabaabec6d7aebfcf",
      "value": " 83.7M/83.7M [00:00&lt;00:00, 117MB/s]"
     }
    },
    "74c63422783a4b9393937c147ab52312": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bdf76418bd444bda805e6bc72c20b41d",
       "IPY_MODEL_c23dbe0f8ae842ac86431dcf6df4e30e",
       "IPY_MODEL_589f98818ece45c08e8b08aba2e10a52"
      ],
      "layout": "IPY_MODEL_cdd6765e1e8c4d9290b925723668ce4d"
     }
    },
    "8b38405fd384473cabaabec6d7aebfcf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8b6fed3895aa41cdafa60d9247406960": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f779fc5df5949c1b78adf7b80af68cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "94ff81cffe4d437e99cb324f03434222": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d203e13b0c340cb83d10cd019b24834": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9fe5f9418c6b4ba59f9cfa672418bdc1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a06e3aeca38a4a40a38c4100c3b99a29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b861e108e0fb44f299d6ee4bf4f31716",
      "max": 52117635,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cfb4327e8b524deab10fdf36251279d4",
      "value": 52117635
     }
    },
    "acc1ea899bc747478bd68d716511bfcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0e9f5322c2534d9fb0e62a3d4064b9dc",
      "placeholder": "​",
      "style": "IPY_MODEL_3b75806d3422402e88fe4a6a34ac0c13",
      "value": " 21.5M/21.5M [00:00&lt;00:00, 131MB/s]"
     }
    },
    "ae75f9cc781541d6864e4c534f0f934d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0e0fc63c332428686df86928b393d4d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7d130944252418cb01436e472e99045": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b861e108e0fb44f299d6ee4bf4f31716": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bdf76418bd444bda805e6bc72c20b41d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd8b2392e0724dbca38117ea7e3d8092",
      "placeholder": "​",
      "style": "IPY_MODEL_b7d130944252418cb01436e472e99045",
      "value": "100%"
     }
    },
    "c203344233064ec2a22bb8ca52475952": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c21f8ec0668041008a7ee7b4243147d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c23dbe0f8ae842ac86431dcf6df4e30e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9fe5f9418c6b4ba59f9cfa672418bdc1",
      "max": 6534387,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_275c1550ed0c4ebfb10623e0cb831f40",
      "value": 6534387
     }
    },
    "cdd6765e1e8c4d9290b925723668ce4d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cfb4327e8b524deab10fdf36251279d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cff8848a7ca448eab31e86871e0d965a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d60b6159cd94445193dcd3c710944f0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_51a52a8f1f9a437091a15d8e4698ba0a",
       "IPY_MODEL_fdd0228e512f4cd1a2437ccb832e784c",
       "IPY_MODEL_740c00fec1534ba39dae12b4c21ba4bc"
      ],
      "layout": "IPY_MODEL_0c6b9533bf5940c88d407d6ab4c2d3ac"
     }
    },
    "d63e0f6aab9c4f39a30077a535aa2fdb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_283fcfb3503f4ac6af092bcbc65539eb",
       "IPY_MODEL_a06e3aeca38a4a40a38c4100c3b99a29",
       "IPY_MODEL_440d6d451ea149c0a88032241b62e89e"
      ],
      "layout": "IPY_MODEL_cff8848a7ca448eab31e86871e0d965a"
     }
    },
    "d8c7c0efc98f435ab4d6472f0b503d22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e01152f2c2c5454d8e03da07a2de2b0b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f13a88dff2da4a009813c56a05da6c47": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_19b7ecc1a0394234bae9059916ba5f2c",
       "IPY_MODEL_296e08bae8b14d03b6ba6aecbf54c0e1",
       "IPY_MODEL_acc1ea899bc747478bd68d716511bfcc"
      ],
      "layout": "IPY_MODEL_b0e0fc63c332428686df86928b393d4d"
     }
    },
    "f3cb7f2d8cab439aaf1be6627d896861": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fd8b2392e0724dbca38117ea7e3d8092": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fdd0228e512f4cd1a2437ccb832e784c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c21f8ec0668041008a7ee7b4243147d9",
      "max": 87769683,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d8c7c0efc98f435ab4d6472f0b503d22",
      "value": 87769683
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
